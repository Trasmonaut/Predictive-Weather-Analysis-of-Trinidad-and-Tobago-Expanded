{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b7854c-2cff-4e40-a51b-63ab08f7f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location URLS\n",
    "locations = [\n",
    "    \"Port%20of%20Spain%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"San%20Fernando%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Arima%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Sangre%20Grande%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Tunapuna-Piarco%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Crown%20Point%2C%20Tobago%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Scarborough%2C%20Tobago%2C%20Trinidad%20and%20Tobago\",    \n",
    "    \"Rio%20Claro%20-%20Mayaro%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Siparia%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Chaguanas%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Diego%20Martin%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Princes%20Town%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Couva-Tabaquite-Talparo%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Point%20Fortin%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"San%20Juan%20-%20Laventille%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\",\n",
    "    \"Penal%20-%20Debe%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\"\n",
    "]\n",
    "\n",
    "#API Keys used to acquire data\n",
    "Keys =  [\"YE8Q69MV94F3F7P7KK22RGSZ3\",\n",
    "         \"GJYFJ3P7MALKURPFM5FNQ6XFU\",\n",
    "         \"CLCPHZRFTSSMC57NMAJZWHJEK\",\n",
    "         \"DX7FS89DEGDLSCES4HBAUL5QN\",\n",
    "         \"ML75W65AKCQB2BKBFGGDWX47W\",\n",
    "         \"PUSVDCB5AR284TUWQ6XSSNF4C\",\n",
    "         \"W9CC59HQJRNJ7Z69J7GJHJFPB\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37cc1b44-3589-4d0f-aa06-4e5da15c6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import csv\n",
    "import codecs\n",
    "\n",
    "# Define base URL and your API key\n",
    "base_url = \"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/\"\n",
    "location = \"Penal%20-%20Debe%2C%20Trinidad%2C%20Trinidad%20and%20Tobago\"\n",
    "loc = \"Penal-Debe\"\n",
    "key = \"DX7FS89DEGDLSCES4HBAUL5QN\"\n",
    "unit_group = \"us\"\n",
    "include = \"days\"\n",
    "content_type = \"csv\"\n",
    "\n",
    "# Define the year range you want to query\n",
    "start_year = 2000\n",
    "end_year = 2025\n",
    "\n",
    "# Function to fetch data for a specific year and save to a CSV file\n",
    "def fetch_weather_data_for_year(year):\n",
    "    url = f\"{base_url}{location}/{year}-01-01/{year}-12-31?unitGroup={unit_group}&include={include}&key={key}&contentType={content_type}\"\n",
    "    \n",
    "    try:\n",
    "        # Make the request to fetch data\n",
    "        ResultBytes = urllib.request.urlopen(url)\n",
    "        \n",
    "        # Parse the result as CSV\n",
    "        CSVText = csv.reader(codecs.iterdecode(ResultBytes, 'utf-8'))\n",
    "        \n",
    "        # Open the CSV file in append mode to add data for each year\n",
    "        with open(f\"weather_data_{loc}.csv\", mode='a', newline='', encoding='utf-8') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Write each row from the CSVText to the file\n",
    "            for row in CSVText:\n",
    "                csv_writer.writerow(row)\n",
    "        \n",
    "        print(f\"Weather data for {year} has been saved to 'weather_data.csv'\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while fetching data for {year}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109627a8-2bf8-440a-9416-3f76e5c84427",
   "metadata": {},
   "source": [
    "Initiall Call of Function to download csv data. Iterated from 2000 to 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ebccf-ee57-4db5-bc16-fa0895e72db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each year in the defined range\n",
    "for year in range(start_year, end_year + 1):\n",
    "    fetch_weather_data_for_year(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a234b7-b136-47e0-a08f-437753ac456b",
   "metadata": {},
   "source": [
    "The initial call was insufficient to acquire all the desired years. It would download a few and skip over certain years inbetween due to daily download limitations for the API key being used. Therefore, the initial function was modified to take in perameters such as a new API key as well as a list with the missing years for them to be acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e9096-0090-48f2-b891-8126d65b8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def fetch_weather_data(year, location, loc, base_url, key, unit_group, include, content_type):\n",
    "    url = f\"{base_url}{location}/{year}-01-01/{year}-12-31?unitGroup={unit_group}&include={include}&key={key}&contentType={content_type}\"\n",
    "    filename = f\"weather_data_{loc}_{year}.csv\"\n",
    "    \n",
    "    for attempt in range(5):  # Try 5 times if necessary\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Weather data for {year} has been saved to {filename}\")\n",
    "                break  # If successful, stop retrying\n",
    "            else:\n",
    "                print(f\"Failed for {year} with status code {response.status_code}\")\n",
    "                time.sleep(20)  # Sleep before retry\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error occurred while fetching data for {year}: {e}\")\n",
    "            time.sleep(20)  # Sleep before retry\n",
    "\n",
    "# Retry only the years where 429 was encountered\n",
    "years_to_retry = [2017, 2018, 2020, 2021, 2022, 2023, 2024, 2025]\n",
    "key = \"ML75W65AKCQB2BKBFGGDWX47W\"\n",
    "\n",
    "for year in years_to_retry:\n",
    "    fetch_weather_data(year, location, loc, base_url, key, unit_group, include, content_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f1b9c-2a4d-4772-b55c-c7d15b855efe",
   "metadata": {},
   "source": [
    "All the files stored for the specific location were then retrieved from the appropriate directory, merged in ascending order (2000 - 2025) and then written to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aaa41c-ca99-4cc3-b4d7-4222b8e6f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Folder where your CSVs are saved\n",
    "folder_path = f\"./Location/{loc}/\"\n",
    "\n",
    "# Step 1: Load all CSVs into one big DataFrame\n",
    "all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "all_dfs = []\n",
    "\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # ğŸ”¥ Skip rows where 'datetime' column is literally 'datetime' (header inside data)\n",
    "    df = df[df['datetime'] != 'datetime']\n",
    "\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Step 2: Combine ALL loaded data\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Step 3: Parse datetime properly\n",
    "combined_df['datetime'] = pd.to_datetime(combined_df['datetime'], errors='coerce')\n",
    "\n",
    "# Step 4: Drop any rows where datetime couldn't be parsed\n",
    "combined_df = combined_df.dropna(subset=['datetime'])\n",
    "\n",
    "# Step 5: Sort data properly by 'datetime'\n",
    "combined_df = combined_df.sort_values('datetime')\n",
    "\n",
    "# Step 6: Filter to only years 2000 to 2025\n",
    "combined_df = combined_df[(combined_df['datetime'].dt.year >= 2000) & (combined_df['datetime'].dt.year <= 2025)]\n",
    "\n",
    "# Step 7: Reset index\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 8: Save to a new CSV\n",
    "combined_df.to_csv(f\"{loc}_weather_data_2000_2025.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… All files successfully combined into '{loc}_weather_data_2000_2025.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0725a-0260-4504-9c09-a5f17ea5127b",
   "metadata": {},
   "source": [
    "The data was read in and cleaned before being re-written to the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b2567d7-707c-43e7-b513-71ec5d51bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{loc}_weather_data_2000_2025.csv')\n",
    "\n",
    "# If a record exists without values for these three variables, then that record has no significant data\n",
    "columns_to_check = ['temp', 'tempmax', 'tempmin']\n",
    "\n",
    "# Drop rows where all of these columns have missing values\n",
    "df_cleaned = df.dropna(subset=columns_to_check, how='all')\n",
    "\n",
    "# Drop unnecessary columns. Columns with no relevance to Trinidad and Tobago (like 'snow'), columns with recurring values, and columns with excessive missing values.\n",
    "df_cleaned = df_cleaned.drop(columns=['snow', 'snowdepth', 'preciptype', 'windgust', 'severerisk'])\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "df_cleaned.to_csv(f'{loc}_weather_data_2000_2025.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
